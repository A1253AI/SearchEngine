{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cd0d61b",
   "metadata": {},
   "source": [
    "**Importing the libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92159d0a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from urllib.parse import unquote\n",
    "import streamlit as st\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import numpy as np\n",
    "import httpx\n",
    "import os\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aec375",
   "metadata": {},
   "source": [
    "**Headers: HTTP header for a web scraper which helps in simulating a real browser when making HTTP requests.  \n",
    "nest_asyncio allows us to run asynchronous code within already running event loop to prevent runtime error.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdd2937",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the nest_asyncio patch\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configuration\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"Referer\": \"https://www.google.com/\",\n",
    "    \"DNT\": \"1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bdca23",
   "metadata": {},
   "source": [
    "**Defining chunk size and number of urls to be fetched to avoid getting out of memory context error.\n",
    "Using Ollama's API to run the embedding model and LLM locally.  \n",
    "We create a directory to store our search results and FAISS Index.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d0e082",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "total_results_to_fetch = 10\n",
    "chunk_size = 1000\n",
    "\n",
    "# Define output paths \n",
    "output_dir = \"search_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "dataframe_out_path = os.path.join(output_dir, \"search_data.csv\")\n",
    "faiss_index_path = os.path.join(output_dir, \"faiss_index.index\")\n",
    "\n",
    "# Ollama configuration\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "OLLAMA_EMBED_MODEL = \"nomic-embed-text\"\n",
    "OLLAMA_LLM_MODEL = \"llama3.2:1b\" # LLM model name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8394c7",
   "metadata": {},
   "source": [
    "**Below fetch function makes an asynchronous HTTP GET request using an aiohttp session. \n",
    "  Includes: params: query parameters for the URL (like q=search+term)\n",
    "  Allows multiple web requests to run concurrently (non-blocking), which speeds up scraping multiple pages.  \n",
    "  The fetch_page_bing/duck_duck_go function --> sends a request to the browser with query and it recieves the HTML content of the search results. In addition we also parse the HTML results with #Beautiful Soup# and  extract the tittle and   url of each result. It returns total 10 results.**  \n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b00c407",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "async def fetch(session, url, params=None):\n",
    "    \"\"\"Fetch content from a URL with optional parameters.\"\"\"\n",
    "    try:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            return await response.text()\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error fetching URL {url}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "async def fetch_page_bing(session, query, page_num, results):\n",
    "    \"\"\"Fetch search results from Bing\"\"\"\n",
    "    try:\n",
    "        offset = (page_num - 1) * 10\n",
    "        bing_url = \"https://www.bing.com/search\"\n",
    "        params = {\"q\": query, \"first\": offset}\n",
    "        \n",
    "        html = await fetch(session, bing_url, params)\n",
    "        \n",
    "        if not html:\n",
    "            return\n",
    "            \n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "        result_blocks = soup.select('li.b_algo')\n",
    "        \n",
    "        for block in result_blocks:\n",
    "            title_elem = block.select_one('h2 a')\n",
    "            if not title_elem:\n",
    "                continue\n",
    "                \n",
    "            title = title_elem.text.strip()\n",
    "            url = title_elem.get('href', '')\n",
    "            \n",
    "            if url and (url.startswith('http://') or url.startswith('https://')):\n",
    "                results.append({\"title\": title, \"links\": url})\n",
    "                \n",
    "            if len(results) >= total_results_to_fetch:\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error parsing Bing search results: {str(e)}\")\n",
    "\n",
    "\n",
    "async def fetch_page_ddg(session, query, page_num, results):\n",
    "    \"\"\"Fetch search results from DuckDuckGo\"\"\"\n",
    "    try:\n",
    "        ddg_url = \"https://html.duckduckgo.com/html/\"\n",
    "        params = {\"q\": query}\n",
    "        \n",
    "        html = await fetch(session, ddg_url, params)\n",
    "        \n",
    "        if not html:\n",
    "            return\n",
    "            \n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "        result_blocks = soup.select('.result')\n",
    "        \n",
    "        for block in result_blocks:\n",
    "            title_elem = block.select_one('.result__title a')\n",
    "            if not title_elem:\n",
    "                continue\n",
    "                \n",
    "            title = title_elem.text.strip()\n",
    "            url = title_elem.get('href', '')\n",
    "            \n",
    "            if url:\n",
    "                url_match = re.search(r'uddg=([^&]+)', url)\n",
    "                if url_match:\n",
    "                    url = unquote(url_match.group(1))\n",
    "                    \n",
    "            if url and (url.startswith('http://') or url.startswith('https://')):\n",
    "                results.append({\"title\": title, \"links\": url})\n",
    "                \n",
    "            if len(results) >= total_results_to_fetch:\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error parsing DuckDuckGo search results: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39892bfa",
   "metadata": {},
   "source": [
    "**The fetch_page provides option to choose from Bing vs DDG.  \n",
    " The get_all_text_from_url function's role is to extract all text content from a url into readable strings of text.\n",
    " The split_text_into_chunks function's role is to split text into chunks of approximately equal size input. It splits the text into sentences using a regular expression that preserves sentence-ending punctuation.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90f3770",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "async def fetch_page(session, query, page_num, results):\n",
    "    \"\"\"Fetch results from selected search engines\"\"\"\n",
    "    try:\n",
    "        search_engine = st.session_state.get(\"search_engine\", \"DuckDuckGo\")\n",
    "        \n",
    "        if search_engine == \"Bing\":\n",
    "            await fetch_page_bing(session, query, page_num, results)\n",
    "        else:\n",
    "            await fetch_page_ddg(session, query, page_num, results)\n",
    "            \n",
    "        if not results and search_engine != \"Bing\":\n",
    "            await fetch_page_bing(session, query, page_num, results)\n",
    "\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error in search results fetching: {str(e)}\")\n",
    "\n",
    "\n",
    "async def get_all_text_from_url(url):\n",
    "    \"\"\"Improved URL content fetching\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            return \"\"\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        for element in soup.select('script, style, nav, footer, header, aside, iframe'):\n",
    "            element.decompose()\n",
    "            \n",
    "        text = soup.get_text(separator='\\n', strip=True)\n",
    "        return ' '.join(text.split()).strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "async def split_text_into_chunks(text, chunk_size):\n",
    "    \"\"\"Split text into chunks with overlap.\"\"\"\n",
    "    if not text or len(text) < 50:\n",
    "        return []\n",
    "        \n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    chunks, current_chunk = [], []\n",
    "    current_length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if current_length + len(sentence) > chunk_size and current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = current_chunk[-2:] + [sentence]\n",
    "            current_length = sum(len(s) for s in current_chunk)\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += len(sentence)\n",
    "    \n",
    "    return chunks if chunks else [text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618a11b7",
   "metadata": {},
   "source": [
    "**The get_embeddings_from_ollama function uses Ollama's api to get vector embeddings from nomic-embed-text model, it   \n",
    "creates vectors from chunks of texts in a 768-dimensional vector.\n",
    "The generate_answer_with_ollama function uses llama 3.2 with the help ollama api to locally run the model.  \n",
    "It takes the query and the results to generate a response based on the prompt.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19104c3b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "async def get_embeddings_from_ollama(text_chunks):\n",
    "    \"\"\"Get embeddings using Ollama\"\"\"\n",
    "    embeddings = []\n",
    "    async with httpx.AsyncClient(timeout=60.0) as client:\n",
    "        for chunk in text_chunks:\n",
    "            if not chunk.strip():\n",
    "                embeddings.append(np.random.uniform(-0.01, 0.01, 768).tolist())\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                response = await client.post(\n",
    "                    f\"{OLLAMA_BASE_URL}/api/embeddings\",\n",
    "                    json={\"model\": OLLAMA_EMBED_MODEL, \"prompt\": chunk[:5000]}\n",
    "                )\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    embeddings.append(data.get(\"embedding\", []))\n",
    "                else:\n",
    "                    embeddings.append(np.random.uniform(-0.01, 0.01, 768).tolist())\n",
    "            except Exception:\n",
    "                embeddings.append(np.random.uniform(-0.01, 0.01, 768).tolist())\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "async def generate_answer_with_ollama(query, context_chunks):\n",
    "    \"\"\"Generate answer using Ollama's LLM\"\"\"\n",
    "    if not context_chunks:\n",
    "        return \"No relevant information found.\"\n",
    "    \n",
    "    combined_chunks = \" \".join(context_chunks)[:12000]\n",
    "    \n",
    "    prompt = f\"\"\"Answer this question based on the context:\n",
    "    Question: {query}\n",
    "    Context: {combined_chunks}\"\"\"\n",
    "    \n",
    "    try:\n",
    "        async with httpx.AsyncClient(timeout=120.0) as client:\n",
    "            response = await client.post(\n",
    "                f\"{OLLAMA_BASE_URL}/api/generate\",\n",
    "                json={\"model\": OLLAMA_LLM_MODEL, \"prompt\": prompt, \"stream\": False}\n",
    "            )\n",
    "            return response.json().get(\"response\", \"Unable to generate answer.\")\n",
    "    except Exception:\n",
    "        return \"Error generating answer.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc93f42",
   "metadata": {},
   "source": [
    "**The function fetch_and_process_data acts as a main function where it initiates search results against the query made and uses multithreading to parallelize text extraction from URLs. Saves all (results) processed data to a CSV file(URLs,text chunks) and also builds a FAISS index for the embeddings and saves it.  \n",
    "The query_vector_store function queries FAISS vector index using a query embedding and returns the top-k most relevant search results. It performs a vector simmilarity search between query vector and each result found, returns distances and indices of top-k results.    \n",
    "A similarity score is used between query and results as a ranking algorithm to determine top-k results we use L2 distance (Euclidean distance) to calculate the distance between query and result in a high dimensonal space. As per score formula the closer distance between vectors represents a higher score,therefore we successfully fetch top-k relevant results with respect to user query.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e5db65",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "async def fetch_and_process_data(search_query):\n",
    "    \"\"\"Process search query and create index\"\"\"\n",
    "    try:\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            results = []\n",
    "            tasks = [fetch_page(session, search_query, page_num, results) for page_num in range(1, 4)]\n",
    "            await asyncio.gather(*tasks)\n",
    "        \n",
    "        if not results:\n",
    "            st.error(\"No search results found.\")\n",
    "            return None\n",
    "            \n",
    "        urls = [result['links'] for result in results]\n",
    "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            texts = await asyncio.gather(*[loop.run_in_executor(executor, get_all_text_from_url, url) for url in urls])\n",
    "        \n",
    "        data = []\n",
    "        for result, text in zip(results, texts):\n",
    "            if not text.strip():\n",
    "                continue\n",
    "                \n",
    "            chunks = split_text_into_chunks(text, chunk_size)\n",
    "            if not chunks:\n",
    "                continue\n",
    "                \n",
    "            embeddings = await get_embeddings_from_ollama(chunks)\n",
    "            for chunk, embedding in zip(chunks, embeddings):\n",
    "                data.append({\n",
    "                    'title': result['title'],\n",
    "                    'url': result['links'],\n",
    "                    'chunk': chunk,\n",
    "                    'embedding': embedding\n",
    "                })\n",
    "        \n",
    "        if not data:\n",
    "            st.error(\"No processable content found.\")\n",
    "            return None\n",
    "            \n",
    "        df = pd.DataFrame(data).drop(columns=['embedding'])\n",
    "        df.to_csv(dataframe_out_path, index=False)\n",
    "        \n",
    "        embeddings = np.array([entry['embedding'] for entry in data], dtype=np.float32)\n",
    "        index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        index.add(embeddings)\n",
    "        faiss.write_index(index, faiss_index_path)\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        st.error(f\"Processing error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "#  query_vector_store function\n",
    "def query_vector_store(query_embedding, k=5):\n",
    "\n",
    "    \"\"\"Query the FAISS index and return top-k unique results\"\"\"\n",
    "    try:\n",
    "        # Load FAISS index\n",
    "        index = faiss.read_index(faiss_index_path)\n",
    "        \n",
    "        # Ensure query embedding is in the correct format\n",
    "        query_embedding = np.array([query_embedding], dtype=np.float32)\n",
    "\n",
    "        # Perform search\n",
    "        distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "        # Load the associated DataFrame\n",
    "        df = pd.read_csv(dataframe_out_path)\n",
    "\n",
    "        results = []\n",
    "        seen_titles = set()  # Track seen titles to avoid duplicates\n",
    "\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if 0 <= idx < len(df):  # Ensure valid index\n",
    "                title = df.iloc[idx]['title']\n",
    "                if title not in seen_titles:\n",
    "                    seen_titles.add(title)\n",
    "                    results.append({\n",
    "                        'title': title,\n",
    "                        'url': df.iloc[idx]['url'],\n",
    "                        'chunk': df.iloc[idx]['chunk'],\n",
    "                        'score': float(1.0 / (1.0 + distances[0][i]))  # Convert distance to similarity score\n",
    "                    })\n",
    "\n",
    "        # Sort results by score (higher is better)\n",
    "        results.sort(key=lambda x: x['score'], reverse=True)\n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying FAISS index: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f592c1",
   "metadata": {},
   "source": [
    "**We create a Streamlit UI Function to test our code and select browser as per user's choice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5203977b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Streamlit UI\n",
    "st.title(\"Search Engine\")\n",
    "\n",
    "# Search engine selection\n",
    "search_engine = st.radio(\n",
    "    \"Select search engine:\",\n",
    "    [\"DuckDuckGo\", \"Bing\"],\n",
    "    horizontal=True\n",
    ")\n",
    "st.session_state[\"search_engine\"] = search_engine\n",
    "\n",
    "query = st.text_input(\"Enter your search query:\")\n",
    "\n",
    "if st.button(\"Search\"):\n",
    "    if not query.strip():\n",
    "        st.warning(\"Please enter a search query\")\n",
    "        st.stop()\n",
    "        \n",
    "    with st.spinner(f\"Searching with {search_engine}...\"):\n",
    "        processed_data = asyncio.run(fetch_and_process_data(query))\n",
    "        \n",
    "        if not processed_data:\n",
    "            st.stop()\n",
    "            \n",
    "        query_embedding = asyncio.run(get_embeddings_from_ollama([query]))[0]\n",
    "        results = query_vector_store(query_embedding)\n",
    "        \n",
    "        if not results:\n",
    "            st.warning(\"No relevant results found.\")\n",
    "            st.stop()\n",
    "            \n",
    "        answer = asyncio.run(generate_answer_with_ollama(query, [r['chunk'] for r in results]))\n",
    "        \n",
    "        st.subheader(\"Answer\")\n",
    "        st.write(answer)\n",
    "        \n",
    "        st.subheader(\"Top Results\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            st.markdown(f\"**{i}. {result['title']}**\")\n",
    "            st.markdown(f\"*Score: {result['score']:.2f}*\")\n",
    "            st.markdown(f\"[{result['url']}]({result['url']})\")\n",
    "            st.write(result['chunk'][:300] + \"...\")\n",
    "            st.write(\"---\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
